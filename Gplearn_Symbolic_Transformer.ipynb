{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instituto Tecnológico y de Estudios Superiores de Occidente\n",
    "## Maestria en Ciencia de Datos\n",
    "## Optimizacion Convexa\n",
    "## HW 04: Constrained optimization and regularization\n",
    "## Symbolic Regressor\n",
    "### Professor: \n",
    "- Dr. Juan Diego Sanchez Torres\n",
    "### Team: \n",
    "- María Elisa Vaca Gómez \n",
    "- Alejandra Paola Galindo Hernández\n",
    "- Jesús Rodrigo Ponce González\n",
    "- Aldo Emmanuel Villareal Palomino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: Symbolic Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates using the **SymbolicTransformer** to generate new non-linear features automatically.\n",
    "\n",
    "Let’s load up the Boston housing dataset and randomly shuffle it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gplearn.genetic import SymbolicTransformer\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.datasets import load_boston\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll use **Ridge Regression** for this example and train our regressor on the first 300 samples, and see how it performs on the unseen final 200 samples. The benchmark to beat is simply Ridge running on the dataset as-is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = check_random_state(0)\n",
    "boston = load_boston()\n",
    "perm = rng.permutation(boston.target.size)\n",
    "boston.data = boston.data[perm]\n",
    "boston.target = boston.target[perm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boston data-set has 506 observations and **13 variables**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model obtained a **$R^2= 0.76$** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7593194530498838\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "est = Ridge()\n",
    "est.fit(boston.data[:300, :], boston.target[:300])\n",
    "R1=est.score(boston.data[300:, :], boston.target[300:])\n",
    "print(est.score(boston.data[300:, :], boston.target[300:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we’ll train our transformer on the same first 300 samples to generate some new features. Let’s use a large population of 2000 individuals over 20 generations. We’ll select the best 100 of these for the `hall_of_fame`, and then use the least-correlated 10 as our new features. A little parsimony should control bloat, but we’ll leave the rest of the evolution options at their defaults. The default `metric='pearson'` is appropriate here since we are using a linear model as the estimator. If we were going to use a tree-based estimator, the Spearman correlation might be interesting to try out too:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `SymbolicTransformer` the algorithm over 20 gerenations, will create 2,000 new variables, then will select the best 100 and consider the 10 least correlated variables as new features. \n",
    "\n",
    "The way that the algorithm create this variables is using the next functions:\n",
    "\n",
    "`function_set = ['add', 'sub', 'mul', 'div', 'sqrt', 'log',\n",
    "                'abs', 'neg', 'inv', 'max', 'min']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    11.04         0.339876        6         0.822502         0.675124     28.38s\n",
      "   1     6.91         0.593562        7         0.836993         0.602468     27.51s\n",
      "   2     5.07         0.730093        8          0.84063         0.704017     26.97s\n",
      "   3     5.22         0.735525        5         0.847019         0.628351     27.51s\n",
      "   4     6.24         0.734679       10         0.856612         0.565138     25.03s\n",
      "   5     8.23         0.721433       18          0.85677         0.728095     27.27s\n",
      "   6    10.20         0.717937       14         0.875233         0.619693     22.76s\n",
      "   7    11.84         0.720667       14         0.875927         0.609363     20.67s\n",
      "   8    12.56         0.733019       27         0.881705         0.390121     20.12s\n",
      "   9    13.61          0.73144       16         0.873285         0.598466     17.57s\n",
      "  10    14.81         0.737687       16         0.873915          0.67127     16.40s\n",
      "  11    14.84          0.73787       21         0.874944         0.467722     14.59s\n",
      "  12    15.40         0.740935       22         0.878053         0.534554     13.61s\n",
      "  13    16.83         0.743265       15         0.874735         0.635764     11.16s\n",
      "  14    17.04         0.741628       13         0.884417         0.493354     10.03s\n",
      "  15    17.02         0.744034       26         0.892236         0.647918      7.85s\n",
      "  16    18.23         0.738467       43         0.879153         0.377872      5.67s\n",
      "  17    18.09         0.722973       16         0.889763         0.508006      3.97s\n",
      "  18    19.58          0.70793       27         0.889402         0.639016      1.99s\n",
      "  19    21.69         0.697116       24         0.888272          0.56025      0.00s\n"
     ]
    }
   ],
   "source": [
    "function_set = ['add', 'sub', 'mul', 'div', 'sqrt', 'log',\n",
    "                'abs', 'neg', 'inv', 'max', 'min']\n",
    "gp = SymbolicTransformer(generations=20, population_size=2000,\n",
    "                         hall_of_fame=100, n_components=10,\n",
    "                         function_set=function_set,\n",
    "                         parsimony_coefficient=0.0005,\n",
    "                         max_samples=0.9, verbose=1,\n",
    "                         random_state=0)\n",
    "\n",
    "gp.fit(boston.data[:300, :], boston.target[:300])\n",
    "gp_features = gp.transform(boston.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result we have a new array with 10 new variables for the 506 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 10)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll combine the original data-set (506 observations and 13 variables) and the one obtained with the `SymbolicTransformer` (506 observations and 10 variables), we'll save this in a array named `new_boston` (506 observations and 23 varaibles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 23)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_boston = np.hstack((boston.data, gp_features))\n",
    "new_boston.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a **Ridge Regression** with this new data-set using the first 300 observations and tested with the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.841837210518192\n"
     ]
    }
   ],
   "source": [
    "est = Ridge()\n",
    "est.fit(new_boston[:300, :], boston.target[:300])\n",
    "R2=est.score(new_boston[300:, :], boston.target[300:])\n",
    "print(est.score(new_boston[300:, :], boston.target[300:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result we obtained a **$R^2 = 0.84$**, we can conclude that using tha data adding the variables created with the `SymbolicTransformer` we increase the score of the model by **0.08**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08251775746830825"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2-R1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
